\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {0}Policies}{1}{section.0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1}List of Collaborators}{1}{subsection.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2}List of Acknowledgments}{1}{subsection.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3}Policies}{1}{subsection.0.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem: Criteria for Choosing a Feature to Split}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Not Splitting}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Splitting}{1}{subsection.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Contingency table for $\Phi '$\relax }}{2}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:tabel1}{{1}{2}{Contingency table for $\Phi '$\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of mutual information between feature and label and error reduction rate for splitting the dataset.\relax }}{2}{figure.caption.3}}
\newlabel{fig:1.3}{{1}{2}{Comparison of mutual information between feature and label and error reduction rate for splitting the dataset.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Mutual Information}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Decision Stumps and Trees}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Built two decision stumps}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Build Decision Tree}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multivariate Decision Tree}{3}{subsection.2.3}}
\newlabel{fig:2.1}{{2}{4}{}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Error for the feature "age"}}}{4}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Error for the feature "salary"}}}{4}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Mutual information for the feature "age"}}}{4}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Mutual information for the feature "salary"}}}{4}{subfigure.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Decision stumps built after searching for the best threshold (Fig \ref  {fig:2.1}).\relax }}{4}{figure.caption.5}}
\newlabel{fig:stumps}{{3}{4}{Decision stumps built after searching for the best threshold (Fig \ref {fig:2.1}).\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Greedily built decision trees. The splits for the upper tree were decided by searching for the best threshold to lower the total error. The splits for the lower tree were motivated by the best threshold corresponding to the largest information gain (mutual information). The label in the graphic for each node depicts the given label (upper row) and the abundance of negative examples (lower row, left value) and positive examples (lower row, right value). The errors for both trees are vanishing for deep nodes.\relax }}{5}{figure.caption.6}}
\newlabel{fig:tree}{{4}{5}{Greedily built decision trees. The splits for the upper tree were decided by searching for the best threshold to lower the total error. The splits for the lower tree were motivated by the best threshold corresponding to the largest information gain (mutual information). The label in the graphic for each node depicts the given label (upper row) and the abundance of negative examples (lower row, left value) and positive examples (lower row, right value). The errors for both trees are vanishing for deep nodes.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Decision tree built by using a multivariate approach.\relax }}{5}{figure.caption.7}}
\newlabel{fig:multi}{{5}{5}{Decision tree built by using a multivariate approach.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Discussion}{5}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}A "Baby" no Free Lunch Theorem}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Number of Inputs}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Number of Function}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Obtaining low expected loss}{6}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{7}{subsection.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation: \textit  {K}-Means}{7}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Result of the K-means routine. Top panel: within-group-sum-of-squares as a function ok K. Bottom panel: mistake rate as a function of K. \relax }}{7}{figure.caption.8}}
\newlabel{fig:Kmeans}{{6}{7}{Result of the K-means routine. Top panel: within-group-sum-of-squares as a function ok K. Bottom panel: mistake rate as a function of K. \relax }{figure.caption.8}{}}
