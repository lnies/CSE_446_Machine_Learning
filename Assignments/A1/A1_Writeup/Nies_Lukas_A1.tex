\documentclass[12pt]{article}
\usepackage{a4}
\usepackage[english]{babel}
\setlength{\parindent}{0.35cm}
\pagestyle{headings}
\usepackage{graphicx}
\usepackage{grffile}
%Multiple picture in one figure
%\usepackage{subfigure}
\usepackage{subfig}
\usepackage{listings}
\usepackage{color}
\usepackage{wrapfig}
%Floating-Umgebungen
\usepackage{float}
%Math-Environment
\usepackage{amsmath}
\usepackage{amssymb}
%Better SI-Units
\usepackage{siunitx}
%Using Appendix
\usepackage[title]{appendix}
%Using URL
\usepackage[hidelinks]{hyperref}
%Using Colored Tables
\usepackage{colortbl}
\newcommand{\gray}{\rowcolor[gray]{.90}}
\usepackage{esvect}
% Build fancy tables
\usepackage{booktabs}
%Configure geometry
\usepackage{geometry}
\geometry{
	a4paper,
	left=3cm,
	right=3cm,
	top=3cm,
	bottom = 3cm,
	}

\lstset{
	language=C++,
	basicstyle=\small\ttfamily,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{green}\ttfamily,
	morecomment=[l][\color{magenta}]{\#},
}


\begin{document}
	
	\title{
		\textbf{\huge{CSE 446: Machine Learning Winter 2018 }} \\[2cm]
		\LARGE{Assignment 1}\\[1cm]
	}
	\author{from \\ Lukas Nies \\ University of Washington}
	\date{01/18/18}
	\clearpage\maketitle\thispagestyle{empty}
	\newpage

	\tableofcontents
	\setcounter{page}{0}
	\newpage
	
	% To start with section 1 labeled as section 0
	\setcounter{section}{-1}
	

\section{Policies}

\subsection{List of Collaborators}

My collaborators were Edith Heiter (discussed Problem 1 and Problem 3) and Julia Hesss (discussed Problem 3 and Problem 4). The development of the answers though was completely independent and individually.

\subsection{List of Acknowledgments}

None.

\subsection{Policies}

I have read and understood these policies.

\section{Problem: Criteria for Choosing a Feature to Split}

We build a tree with the dataset $D$ consisting $n$ negative examples (label 0) and $p$ positive examples (label 1). 

\subsection{Not Splitting}

If we are at the bottom of our decision tree and don't have any features left to split, consider the subset $D'$ of data $D$ with $n'$ negative and $p'$ positive examples. The smallest number of mistakes we can make in this subset is given by

\begin{align}
	\text{err}(D') = \min(n',p') = 
	\begin{cases}
		p' & \text{if } p'< n' \\
		n' & \text{if } n'< p'
	\end{cases}
\end{align}

To achieve the minimum the node must be labeled accordingly, with $1$ if $n'< p'$ or with $0$ if $n'> p'$. 

\subsection{Splitting}

Now we have a new feature $\Phi$ which splits the subsection $D'$ according to the contingency table:
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|}
		\toprule
		y     & \multicolumn{2}{c|}{$\Phi$} \\
		& \multicolumn{1}{c}{0} & 1 \\
		\cmidrule{2-3}    0     & $n_0$    & $n_1$ \\
		\cmidrule{2-3}    1     & $p_0$    & $p_1$ \\
		\bottomrule
	\end{tabular}%
	\caption{Contingency table for $\Phi'$}
	\label{tab:tabel1}%
\end{table}%
By splitting we generate two new sub-nodes: $(n_0,p_0)$ and $(n_1,p_1)$. The error for splitting is given by the sum of the errors of both nodes
\begin{align}
\text{err}(D') = \min(n_0,p_0) + \min(n_1,p_1),
\end{align}
where $n'=n_0 + n_1$ and $p'=n_1+p_1$. The error reduction rate ($\text{err}\_\text{red}$) is given by the reduction of error if comparing the error of "not splitting" with the error of "splitting", divided by the total number of examples in node $D'$:
\begin{align}
\text{\text{err}\_\text{red}}(D'): \frac{\min(n',p')-\left(\min(n_0,p_0) + \min(n_1,p_1)\right)}{\lvert D' \rvert}.
\end{align} 
Consider the maximal possible error (in this case for binary data) when $n'=p'=0.5 \lvert D' \rvert$
\begin{align}
\text{err}_\text{max}(D')=\min(n',p')=0.5\lvert D' \rvert,
\end{align}
then the maximal possible error reduction is given by 
\begin{align}
\text{\text{err}\_\text{red}}(D'): \frac{\min(0.5 \lvert D'\rvert,0.5 \lvert D' \rvert)-\left(\min(n_0,p_0) + \min(n_1,p_1)\right)}{\lvert D' \rvert} = 0.5,
\end{align}
where either $p_0=0$ or $n_0=0$ and $p_1=0$ or $n_1=0$ (maximal information gain).

\subsection{Mutual Information}

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.65\linewidth]{../Problem_1/Figure_1.png}
	\caption{Comparison of mutual information between feature and label and error reduction rate for splitting the dataset.}
	\label{fig:1.3}
\end{figure}

The mutual information is a measure of information gain when splitting a dataset. In figure \ref{fig:1.3} the mutual information and the error reduction rate are plotted for problem $1.3$. Both functions are symmetric around index $250$ which represents equally distributed examples in form of $n_0=n_1=p_0=p_1=250$ . \par 
As we showed in the previous part of this problem, the error reduction rate is maximal for the case where the error of a node is $0$ after splitting. Corresponding to this the information gain in form of the mutual information is highest. \par
For increasing indices the information gain and the error reduction rate decrease until a minimum in $i=250$, afterwards rising again.    

\section{Decision Stumps and Trees}

\subsection{Built two decision stumps}

In figure \ref{fig:2.1} the comparison of of error rates and mutual information for different threshold splits and features are plotted. \par 
The top row shows the error as a function threshold. The feature "age" generates two minimums corresponding to two possible splits to minimize the error when splitting. For feature "salary" we get only one minimum but the error reduction is larger. \par 
The mutual information is plotted in the bottom row. Since the mutual information gives the information gain for a split we have to search for maximums. Both features give one maximal values which we can choose as a threshold to split the node. \par
In figure \ref{fig:stumps} the decision stumps are plotted for the best thresholds found in figure \ref{fig:2.1}. The error rate depends on choosing a feature to split and on the method to calculate the best threshold. The best errors after splitting only once were achieved by choosing mutual information as a criteria (both features) or choosing "salary" when counting errors (again, see figure \ref{fig:stumps}).

\begin{figure}[t!]
	\centering
	\subfloat[Error for the feature "age"] {\includegraphics[width=0.45\textwidth]{../Problem_2/Figure_2.1_age_normal.png}}
	\hfill
	\subfloat[Error for the feature "salary"] {\includegraphics[width=0.45\textwidth]{../Problem_2/Figure_2.1_sal_normal.png}}
	\hfill
	\subfloat[Mutual information for the feature "age"] {\includegraphics[width=0.45\textwidth]{../Problem_2/Figure_2.1_age_mut.png}}
	\hfill
	\subfloat[Mutual information for the feature "salary"] {\includegraphics[width=0.45\textwidth]{../Problem_2/Figure_2.1_sal_mut.png}}
	\caption[]{Comparison of error rates and mutual information for different threshold splits and features. For generating this data a script has been written to scan through different thresholds with a fine reasonable step width. The error respectively mutual information was calculated as shown in problem 1. }
	\label{fig:2.1}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../Problem_2/Figure_2.1_stamps.PNG}
	\caption{Decision stumps built after searching for the best threshold (Fig \ref{fig:2.1}).}
	\label{fig:stumps}
\end{figure}

\subsection{Build Decision Tree}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{../Problem_2/2.2.png}
	\caption{Greedily built decision trees. The splits for the upper tree were decided by searching for the best threshold to lower the total error. The splits for the lower tree were motivated by the best threshold corresponding to the largest information gain (mutual information). The label in the graphic for each node depicts the given label (upper row) and the abundance of negative examples (lower row, left value) and positive examples (lower row, right value). The errors for both trees are vanishing for deep nodes.}
	\label{fig:tree}
\end{figure}



In figure \ref{fig:tree} the greedily built decision trees are shown. We can see that the lower tree (built by using mutual information) only needs six nodes to reach an overall error of zero on the training set while the upper tree (built by counting errors) needs ten nodes. 

\subsection{Multivariate Decision Tree}

To built the multivariate decision tree we can utilize some of the basic geometric foundations used in the Perceptron algorithm. We have to learn the function
\begin{align}
f(\vec{x})=\left[\sum_{d=1}^{2}w_dx_d\right]-1=w_1x_\text{age}+w_2x_\text{salary}-1.
\end{align}
Since we work in two dimensions it's most convenient to just "guess" the hyperplane of the data which is in this case of dimension one, a line. Let $\vec{h}$ be the vector of this hyperplane. By drawing a straight line to separate negative and positive examples we can calculate the slope. This yields in 
\begin{align*}
\vec{h} = \begin{pmatrix}35 \\ 40000\end{pmatrix}
\end{align*}
where the first component corresponds to the age and the second component corresponds to the salary. According the the Perceptron the weight vector $\vec{w}$ is perpendicular to the hyperplane. To get $\vec{w}$ we can perform a rotation of $\vec{h}$ by applying a 2D rotation matrix:
\begin{align*}
\vec{w}=\text{rot}(90^{\circ})\cdot \vec{h} = 
	\begin{pmatrix}
		\cos(90^{\circ}) & \sin(90^{\circ}) \\
		-\sin(90^{\circ}) & \cos(90^{\circ}) 
	\end{pmatrix} \cdot
	\begin{pmatrix}
		35 \\ 40000
	\end{pmatrix} =
	\begin{pmatrix}
		40000 \\ -35
	\end{pmatrix}
\end{align*}
We can norm this vector to get some smaller values (not necessary but for the sake of readability):
\begin{align*}
\vec{w}_n = \frac{1}{\sqrt{(40000)^2+(-35)^2}}
\begin{pmatrix}
	40000 \\ -35
\end{pmatrix} =
\begin{pmatrix}
	0.999999617 \\ -8.75\times 10^{-4}
\end{pmatrix}
\end{align*}
This weight vector (which we "learned" by manually looking at the data) gives us now the hypothesis $f$ to test on the trainings set. The decision now is
\begin{align}
f(\vec{x})=\text{sign}\left[0.99999961\cdot x_\text{age}-8.75\times 10^{-4}\cdot x_\text{salary}-1\right].
\end{align}
In figure \ref{fig:multi} the multivariate approach is shown. We can see that it only takes one split to get an error of zero (note: this might not be true for real life data, in this case it is just coincidence of the constructed data). Because of this, the information gain is $1$.  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.35\linewidth]{../Problem_2/Figure_2.3.png}
	\caption{Decision tree built by using a multivariate approach.}
	\label{fig:multi}
\end{figure}

\subsection{Discussion}

Some thoughts on univariate and multivariate decision trees: considering 2D data (like in our example above) we can clearly see that the multivariate approach in this cases works very efficiently. This is because the data points are easily separable by one straight line. If the data appears to be clustered then this algorithm works very efficient. \par 
However, if the data is mixed and no distinct clustering can be found then this approach might be very inefficient. In this case univariate decision trees might be a better choice.   

\section{A "Baby" no Free Lunch Theorem}

Considering a function
\begin{align}
f:\{0,1\}^m\rightarrow \{0,1\}
\end{align}
mapping a binary string of length $m$ to a label, either $0$ or $1$. 

\subsection{The Number of Inputs}

The number of possible input strings is given by 
\begin{align}
N_\text{input}=2^m 
\end{align}
and for the length $m=5$ we have $32$ different input strings.

\subsection{The Number of Function}

Each bit in input string $\{0,1\}^m$ can be mapped to two possible labels, either $0$ or $1$. The input bit itself can be either $0$ or $1$, giving now two times two possibilities of mapping. Since we have $m$ input bits we get 
\begin{align}
	N_\text{functions}=\left(2\cdot 2\right)^{m}=(2^2)^m=2^{2m}
\end{align}
mappings functions. For $m=5$ we have 1024 different functions. 

\subsection{Obtaining low expected loss}

If we consider Alice to give Bob only uniformly and randomly distributed distinct inputs from the set of inputs $\{0,1\}^m$ then Bob can learn for each new distinct input, which he has never seen before, a new mapping from this input to the true label. So far, so good. If Bob wants to learn Alice' function with an expected misclassification rate of 50\% or better, then he hast to see at least 50\% of all possible distinct inputs out of the input set $\{0,1\}^m$. This corresponds to $\mathcal{O}(\frac{1}{2}\cdot 2^m)$ distinct inputs. For $m=5$ this will be around $16$ distinct inputs. But as always: more is better!\par 
This is only true if the test set which tests Bob's knowledge is equally generated as the uniformly distributed trainings set. If Alice gives Bob only inputs for testing which he has never seen before than his error rate will never be better then simply guessing. 

\subsection{Discussion}

The only possible way Bob has learned Alice' function with less inputs then discussed in previous section is that he knows the fundamentally distribution after how Alice generates the randomly distributed distinct inputs. By this knowledge Bob is able to predict the distinct inputs with correct labels with a misclassification rate better than guessing. He can convince Janet by correctly predicting further input samples provided by Alice without ever having seen them before.

\section{Implementation: \textit{K}-Means}
The script loops over $K \in \{1,2,\dots,10\}$, randomly initializes five times for each $K$ and returns the result with the lowest within-group sum of squares. \par 
The upper panel of figure \ref{fig:Kmeans} shows the distortion as a function of K. For an increasing number of centroids the distortion decreases. This is expected since we need at least four centroids to get a good prediction for the four possible labels. We can assume that the distortion converges for higher numbers of K around 150000 to 200000.\par 
The lower panel shows the mistake rate as a function of K. Again, the error rate decreases with increasing numbers of centroids. This also is due to the number of possible labels. The rate seems to converge around 20\%. \par 
Both graphs have the same shape: the performance gets better for larger numbers of K. This is as expected. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{../Problem_4/Run_5.png}
	\caption{Result of the K-means routine. Top panel: within-group-sum-of-squares as a function ok K. Bottom panel: mistake rate as a function of K. }
	\label{fig:Kmeans}
\end{figure}



%\chapter*{Bibliography}
%\addcontentsline{toc}{chapter}{Bibliography}%	

%\bibliographystyle{unsrt}
%\bibliography{./bib}





\end{document}  