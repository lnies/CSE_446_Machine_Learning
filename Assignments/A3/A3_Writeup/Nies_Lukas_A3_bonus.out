\BOOKMARK [1][-]{section.0}{Policies}{}% 1
\BOOKMARK [2][-]{subsection.0.1}{List of Collaborators}{section.0}% 2
\BOOKMARK [2][-]{subsection.0.2}{List of Acknowledgments}{section.0}% 3
\BOOKMARK [2][-]{subsection.0.3}{Policies}{section.0}% 4
\BOOKMARK [2][-]{subsection.0.4}{Note: Bonus included!}{section.0}% 5
\BOOKMARK [1][-]{section.1}{Problem: Linear Regression on MNIST}{}% 6
\BOOKMARK [2][-]{subsection.1.1}{Closed Form Estimator}{section.1}% 7
\BOOKMARK [2][-]{subsection.1.2}{Linear regression using gradient descent}{section.1}% 8
\BOOKMARK [2][-]{subsection.1.3}{Linear Regression Using Stochastic Gradient Descent}{section.1}% 9
\BOOKMARK [2][-]{subsection.1.4}{BONUS: Mini-batch stochastic gradient descent}{section.1}% 10
\BOOKMARK [2][-]{subsection.1.5}{BONUS: Using polynomial features}{section.1}% 11
\BOOKMARK [1][-]{section.2}{Binary Classification with Logistic Regression}{}% 12
\BOOKMARK [2][-]{subsection.2.1}{BONUS: Log loss, applied}{section.2}% 13
\BOOKMARK [1][-]{section.3}{Multi-Class classification using Least Squares}{}% 14
\BOOKMARK [2][-]{subsection.3.1}{"One vs. all Classification" with Linear Regression}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.2}{BONUS: Matrix derivative}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.3}{BONUS: Softmax}{section.3}% 17
\BOOKMARK [1][-]{section.4}{Probability and Maximum Likelihood Estimation}{}% 18
\BOOKMARK [2][-]{subsection.4.1}{Probability Review}{section.4}% 19
\BOOKMARK [2][-]{subsection.4.2}{Maximum Likelihood Estimation}{section.4}% 20
\BOOKMARK [1][-]{section.5}{BONUS: State of the art on MNIST}{}% 21
\BOOKMARK [2][-]{subsection.5.1}{BONUS: Start small \(k=5000\)}{section.5}% 22
\BOOKMARK [2][-]{subsection.5.2}{BONUS: Go big \(k=60000\)}{section.5}% 23
\BOOKMARK [0][-]{Item.53}{Bibliography}{}% 24
