\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {0}Policies}{1}{section.0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1}List of Collaborators}{1}{subsection.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2}List of Acknowledgments}{1}{subsection.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3}Policies}{1}{subsection.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4}Note: Bonus included!}{1}{subsection.0.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem: Linear Regression on MNIST}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Closed Form Estimator}{2}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear regression using gradient descent}{2}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.2}{{1}{3}{Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Linear Regression Using Stochastic Gradient Descent}{3}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}\leavevmode {\color {red}BONUS: Mini-batch stochastic gradient descent}}{3}{subsection.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical axis) and misclassification loss in percent (right) for the stochastic gradient descent algorithm. The horizontal axis shows the iteration steps for every 500th step. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-1}$ and $\eta =0.005$ were chosen.\relax }}{4}{figure.caption.3}}
\newlabel{fig:1.3}{{2}{4}{Plot of averaged squared errors (left, note the logarithmic vertical axis) and misclassification loss in percent (right) for the stochastic gradient descent algorithm. The horizontal axis shows the iteration steps for every 500th step. For generating the plots, $\eta = \frac {1}{4}\times 10^{-1}$ and $\eta =0.005$ were chosen.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical scale) and misclassification loss in percent (right) for the mini batch stochastic gradient descent algorithm with $m=100$. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-2}$ and $\lambda =0.005$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }}{4}{figure.caption.4}}
\newlabel{fig:1.4}{{3}{4}{Plot of averaged squared errors (left, note the logarithmic vertical scale) and misclassification loss in percent (right) for the mini batch stochastic gradient descent algorithm with $m=100$. For generating the plots, $\eta = \frac {1}{4}\times 10^{-2}$ and $\lambda =0.005$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}\leavevmode {\color {red}BONUS: Using polynomial features}}{4}{subsection.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical scale) and misclassification loss in percent (right) for the normal stochastic gradient descent algorithm with polynomial features. For generating the plots, $\eta = \times 10^{-5}$ and $\lambda =0.0001$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }}{5}{figure.caption.5}}
\newlabel{fig:1.5}{{4}{5}{Plot of averaged squared errors (left, note the logarithmic vertical scale) and misclassification loss in percent (right) for the normal stochastic gradient descent algorithm with polynomial features. For generating the plots, $\eta = \times 10^{-5}$ and $\lambda =0.0001$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Binary Classification with Logistic Regression}{6}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\leavevmode {\color {red}BONUS: Log loss, applied}}{7}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical scale), the misclassification loss in percent (middle) and the log loss (right) for the mini batch stochastic gradient descent algorithm with normal features. For generating the plots, $\eta = \times \frac  {1}{4}\times 10^{-2}$ and $\lambda =0.01$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }}{7}{figure.caption.6}}
\newlabel{fig:2.1}{{5}{7}{Plot of averaged squared errors (left, note the logarithmic vertical scale), the misclassification loss in percent (middle) and the log loss (right) for the mini batch stochastic gradient descent algorithm with normal features. For generating the plots, $\eta = \times \frac {1}{4}\times 10^{-2}$ and $\lambda =0.01$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical scale), the misclassification loss in percent (middle) and the log loss (right) for the mini batch stochastic gradient descent algorithm with normal features. For generating the plots, $\eta = \times \frac  {1}{4}\times 10^{-2}$ and $\lambda =0.01$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }}{8}{figure.caption.7}}
\newlabel{fig:2.2}{{6}{8}{Plot of averaged squared errors (left, note the logarithmic vertical scale), the misclassification loss in percent (middle) and the log loss (right) for the mini batch stochastic gradient descent algorithm with normal features. For generating the plots, $\eta = \times \frac {1}{4}\times 10^{-2}$ and $\lambda =0.01$ were chosen. The horizontal axis shows the iteration steps for every 500th step.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-Class classification using Least Squares}{9}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}"One vs. all Classification" with Linear Regression}{9}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }}{9}{figure.caption.8}}
\newlabel{fig:3.1}{{7}{9}{Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\leavevmode {\color {red}BONUS: Matrix derivative}}{9}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\leavevmode {\color {red}BONUS: Softmax}}{10}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Probability and Maximum Likelihood Estimation}{11}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Probability Review}{11}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Maximum Likelihood Estimation}{12}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}BONUS: State of the art on MNIST}{13}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\leavevmode {\color {red}BONUS: Start small ($k=5000$)}}{13}{subsection.5.1}}
\bibstyle{unsrt}
\bibdata{./bib}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\leavevmode {\color {red}BONUS: Go big ($k=60000$)}}{14}{subsection.5.2}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{14}{Item.53}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Case $k=5000$. Plot of averaged squared errors (left), the norm of the weight vector (right) and misclassification loss in percent (middle). For generating the plots, $\eta = 10^{-3}$ and $\lambda =10^{-2}$ were chosen.\relax }}{15}{figure.caption.11}}
\newlabel{fig:5.1}{{8}{15}{Case $k=5000$. Plot of averaged squared errors (left), the norm of the weight vector (right) and misclassification loss in percent (middle). For generating the plots, $\eta = 10^{-3}$ and $\lambda =10^{-2}$ were chosen.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Case $k=60000$. Plot of averaged squared errors (left), the norm of the weight vector (right) and misclassification loss in percent (middle). For generating the plots, $\eta = 10^{-3}$ and $\lambda =10^{-2}$ were chosen.\relax }}{15}{figure.caption.12}}
\newlabel{fig:5.2}{{9}{15}{Case $k=60000$. Plot of averaged squared errors (left), the norm of the weight vector (right) and misclassification loss in percent (middle). For generating the plots, $\eta = 10^{-3}$ and $\lambda =10^{-2}$ were chosen.\relax }{figure.caption.12}{}}
