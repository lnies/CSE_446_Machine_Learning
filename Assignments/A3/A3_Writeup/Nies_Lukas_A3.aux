\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {0}Policies}{1}{section.0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1}List of Collaborators}{1}{subsection.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2}List of Acknowledgments}{1}{subsection.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3}Policies}{1}{subsection.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4}Note: Bonus not included!}{1}{subsection.0.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem: Linear Regression on MNIST}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Closed Form Estimator}{2}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear regression using gradient descent}{2}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.2}{{1}{3}{Plot of averaged squared errors (left) and misclassification loss in percent (right) for the gradient descent algorithm. For generating the plots, $\eta = \frac {1}{4}\times 10^{-1}$ and $\eta =10^{-2}$ were chosen.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Linear Regression Using Stochastic Gradient Descent}{3}{subsection.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of averaged squared errors (left, note the logarithmic vertical axis) and misclassification loss in percent (right) for the stochastic gradient descent algorithm. The horizontal axis shows the iteration steps for every 500th step. For generating the plots, $\eta = \frac  {1}{4}\times 10^{-1}$ and $\eta =0.005$ were chosen.\relax }}{4}{figure.caption.3}}
\newlabel{fig:1.3}{{2}{4}{Plot of averaged squared errors (left, note the logarithmic vertical axis) and misclassification loss in percent (right) for the stochastic gradient descent algorithm. The horizontal axis shows the iteration steps for every 500th step. For generating the plots, $\eta = \frac {1}{4}\times 10^{-1}$ and $\eta =0.005$ were chosen.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Binary Classification with Logistic Regression}{4}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-Class classification using Least Squares}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}"One vs. all Classification" with Linear Regression}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Probability and Maximum Likelihood Estimation}{7}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Probability Review}{7}{subsection.4.1}}
\bibstyle{unsrt}
\bibdata{./bib}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{8}{Item.25}}
